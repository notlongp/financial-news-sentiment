{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction Using Machine Learning - Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For part 4, I was able to applied Natural Language Processing (NLP) models and libraries to evaluate the sentiment of each article's headline (for all 3 major news platforms) and preview (for CNBC and Reuters only). \n",
    "In this part 5, I am going to apply Machine Learning and Deep Learning models to predict the sentiment of each headline and preview based on the tokenized version of said headline/preview. Throughout part 5, I am using data collected from https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news/kernels as the training and validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifically, in part 5.1, I am going to tokenized the texts from the dataset, categorizing their sentiments (negative, neutral, positive) and applying popular Machine Learning models to see which model yields the best result. This result is saved to the final dataframe for my conclusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Long's\n",
      "[nltk_data]     XPS13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'corpus' (list)\n",
      "Stored 'cv' (CountVectorizer)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './lib')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils\n",
    "from sentiment_module import tokenize_stem\n",
    "\n",
    "df = pd.read_csv(\"./data/dataset.csv\", header = None, encoding='latin-1', names=[\"Sentiment\", \"Headlines\"])\n",
    "\n",
    "corpus = []\n",
    "for item in df['Headlines']:\n",
    "    corpus.append(tokenize_stem(item))\n",
    "%store corpus\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "y = df.iloc[:, 0].values\n",
    "%store cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform column y to categorical data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [2 1]\n",
      " [1 1]\n",
      " ...\n",
      " [0 0]\n",
      " [1 0]\n",
      " [1 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\long's xps13\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logistic_c = LogisticRegression()\n",
    "logistic_c.fit(X_train, y_train)\n",
    "y_pred = logistic_c.predict(X_test)\n",
    "print(np.concatenate((y_pred.reshape(len(y_pred), 1), y_test.reshape(len(y_test), 1)), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 71  41  16]\n",
      " [ 28 464  83]\n",
      " [ 24  99 144]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.87250961]\n",
      " [0.87250961 1.        ]]\n",
      "\n",
      "0.7\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import f1_score\n",
    "print(cosine_similarity([y_test, y_pred]))\n",
    "print()\n",
    "print(f1_score(y_test, y_pred, zero_division=1, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K - Nearest Neighbors (5 neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn_c = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n",
    "knn_c.fit(X_train, y_train)\n",
    "y_pred = knn_c.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 60  30  38]\n",
      " [ 26 255 294]\n",
      " [ 19  83 165]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.84830087]\n",
      " [0.84830087 1.        ]]\n",
      "\n",
      "0.4948453608247423\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([y_test, y_pred]))\n",
    "print()\n",
    "print(f1_score(y_test, y_pred, zero_division=1, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_c = SVC(kernel = 'linear', random_state = 0)\n",
    "svm_c.fit(X_train, y_train)\n",
    "y_pred = svm_c.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 70  41  17]\n",
      " [ 73 429  73]\n",
      " [ 44 107 116]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.82587291]\n",
      " [0.82587291 1.        ]]\n",
      "\n",
      "0.634020618556701\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([y_test, y_pred]))\n",
    "print()\n",
    "print(f1_score(y_test, y_pred, zero_division=1, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernel SVM (Gaussian RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "rbf_c = SVC(kernel = 'rbf', random_state = 0)\n",
    "rbf_c.fit(X_train, y_train)\n",
    "y_pred = rbf_c.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 31  77  20]\n",
      " [  2 569   4]\n",
      " [  0 182  85]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.88890799]\n",
      " [0.88890799 1.        ]]\n",
      "\n",
      "0.7061855670103093\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([y_test, y_pred]))\n",
    "print()\n",
    "print(f1_score(y_test, y_pred, zero_division=1, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "bayes_c = GaussianNB()\n",
    "bayes_c.fit(X_train, y_train)\n",
    "y_pred = bayes_c.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 70  24  34]\n",
      " [ 81 296 198]\n",
      " [ 68  78 121]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.77240692]\n",
      " [0.77240692 1.        ]]\n",
      "\n",
      "0.5020618556701031\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([y_test, y_pred]))\n",
    "print()\n",
    "print(f1_score(y_test, y_pred, zero_division=1, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree_c = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n",
    "tree_c.fit(X_train, y_train)\n",
    "y_pred = tree_c.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 59  49  20]\n",
      " [ 20 473  82]\n",
      " [ 22 101 144]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.87082266]\n",
      " [0.87082266 1.        ]]\n",
      "\n",
      "0.6969072164948453\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([y_test, y_pred]))\n",
    "print()\n",
    "print(f1_score(y_test, y_pred, zero_division=1, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest_c = RandomForestClassifier(n_estimators = 10, random_state = 0)\n",
    "forest_c.fit(X_train, y_train)\n",
    "y_pred = forest_c.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 44  63  21]\n",
      " [  7 537  31]\n",
      " [ 12 146 109]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.87810154]\n",
      " [0.87810154 1.        ]]\n",
      "\n",
      "0.711340206185567\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([y_test, y_pred]))\n",
    "print()\n",
    "print(f1_score(y_test, y_pred, zero_division=1, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "classifier = xgb.XGBClassifier()\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 68,  49,  11],\n",
       "       [  6, 539,  30],\n",
       "       [ 10, 116, 141]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.90946066]\n",
      " [0.90946066 1.        ]]\n",
      "\n",
      "0.7711340206185567\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity([y_test, y_pred]))\n",
    "print()\n",
    "print(f1_score(y_test, y_pred, zero_division=1, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through the application of many Machine Learning models, I can conclude that XGBoost Classifier yields the best result. In the next part, I will use this model to predict financial news headlines I collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'xgb_classifier' (XGBClassifier)\n"
     ]
    }
   ],
   "source": [
    "xgb_classifier = classifier\n",
    "%store xgb_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
