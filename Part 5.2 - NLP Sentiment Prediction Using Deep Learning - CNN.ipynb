{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Prediction Using Deep Learning - Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Headlines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4841</th>\n",
       "      <td>0</td>\n",
       "      <td>LONDON MarketWatch -- Share prices ended lower...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4842</th>\n",
       "      <td>1</td>\n",
       "      <td>Rinkuskiai 's beer sales fell by 6.5 per cent ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4843</th>\n",
       "      <td>0</td>\n",
       "      <td>Operating profit fell to EUR 35.4 mn from EUR ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4844</th>\n",
       "      <td>0</td>\n",
       "      <td>Net sales of the Paper segment decreased to EU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4845</th>\n",
       "      <td>0</td>\n",
       "      <td>Sales in Finland decreased by 10.5 % in Januar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4846 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment                                          Headlines\n",
       "0             1  According to Gran , the company has no plans t...\n",
       "1             1  Technopolis plans to develop in stages an area...\n",
       "2             0  The international electronic industry company ...\n",
       "3             2  With the new production plant the company woul...\n",
       "4             2  According to the company 's updated strategy f...\n",
       "...         ...                                                ...\n",
       "4841          0  LONDON MarketWatch -- Share prices ended lower...\n",
       "4842          1  Rinkuskiai 's beer sales fell by 6.5 per cent ...\n",
       "4843          0  Operating profit fell to EUR 35.4 mn from EUR ...\n",
       "4844          0  Net sales of the Paper segment decreased to EU...\n",
       "4845          0  Sales in Finland decreased by 10.5 % in Januar...\n",
       "\n",
       "[4846 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/dataset.csv\", header = None, encoding='latin-1', names=[\"Sentiment\", \"Headlines\"])\n",
    "df['Sentiment'] = df['Sentiment'].replace(\"negative\",0).replace(\"neutral\",1).replace(\"positive\",2)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2879\n",
       "2    1363\n",
       "0     604\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Long's\n",
      "[nltk_data]     XPS13\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './lib')\n",
    "from sentiment_module import *\n",
    "\n",
    "df['Splitted'] = df['Headlines'].apply(lambda x: cleaning_data(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Headlines</th>\n",
       "      <th>Splitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>According to Gran , the company has no plans t...</td>\n",
       "      <td>[according, to, gran, the, company, has, no, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Technopolis plans to develop in stages an area...</td>\n",
       "      <td>[technopolis, plans, to, develop, in, stages, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>The international electronic industry company ...</td>\n",
       "      <td>[the, international, electronic, industry, com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>With the new production plant the company woul...</td>\n",
       "      <td>[with, the, new, production, plant, the, compa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>According to the company 's updated strategy f...</td>\n",
       "      <td>[according, to, the, company, s, updated, stra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Sentiment                                          Headlines  \\\n",
       "0          1  According to Gran , the company has no plans t...   \n",
       "1          1  Technopolis plans to develop in stages an area...   \n",
       "2          0  The international electronic industry company ...   \n",
       "3          2  With the new production plant the company woul...   \n",
       "4          2  According to the company 's updated strategy f...   \n",
       "\n",
       "                                            Splitted  \n",
       "0  [according, to, gran, the, company, has, no, p...  \n",
       "1  [technopolis, plans, to, develop, in, stages, ...  \n",
       "2  [the, international, electronic, industry, com...  \n",
       "3  [with, the, new, production, plant, the, compa...  \n",
       "4  [according, to, the, company, s, updated, stra...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_words = [word for tokens in df_train['Splitted'] for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'third',\n",
       " 'quarter',\n",
       " 'result',\n",
       " 'also',\n",
       " 'includes',\n",
       " 'a',\n",
       " 'euro',\n",
       " 'provision',\n",
       " 'for']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_words = [word for tokens in df_test['Splitted'] for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'bristol',\n",
       " 'port',\n",
       " 'company',\n",
       " 'has',\n",
       " 'sealed',\n",
       " 'a',\n",
       " 'one',\n",
       " 'million',\n",
       " 'pound']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Google News Word2Vec model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "word2vec_path = './data/GoogleNews-vectors-negative300.bin.gz'\n",
    "word2vec = models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing & Pad Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=len(training_words), char_level=False)\n",
    "tokenizer.fit_on_texts(df_train['Headlines'].tolist())\n",
    "train_sequences = tokenizer.texts_to_sequences(df_train['Headlines'].tolist())\n",
    "train_word_index = tokenizer.word_index\n",
    "max_seq_len = max([len(x) for x in train_sequences])\n",
    "\n",
    "# Pad Sequence\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3876, 71)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(df_test['Headlines'].tolist())\n",
    "\n",
    "# Pad Sequence\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_embeddings = np.zeros((len(train_word_index)+1, 300))\n",
    "for word,index in train_word_index.items():\n",
    "    train_embeddings[index,:] = word2vec[word] if word in word2vec else np.random.rand(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9020, 300)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers.embeddings import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "X_train = train_data\n",
    "# y_train = df_train['Sentiment']\n",
    "X_test = test_data\n",
    "# y_test = df_test['Sentiment']\n",
    "y_train = np_utils.to_categorical(df_train['Sentiment'], num_classes=3)\n",
    "y_test = np_utils.to_categorical(df_test['Sentiment'], num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "122/122 [==============================] - 3s 21ms/step - loss: 0.8322 - accuracy: 0.6321\n",
      "Epoch 2/10\n",
      "122/122 [==============================] - 3s 21ms/step - loss: 0.5870 - accuracy: 0.7544\n",
      "Epoch 3/10\n",
      "122/122 [==============================] - 3s 24ms/step - loss: 0.3912 - accuracy: 0.8511\n",
      "Epoch 4/10\n",
      "122/122 [==============================] - 3s 25ms/step - loss: 0.2371 - accuracy: 0.9213\n",
      "Epoch 5/10\n",
      "122/122 [==============================] - 3s 27ms/step - loss: 0.1258 - accuracy: 0.9678\n",
      "Epoch 6/10\n",
      "122/122 [==============================] - 3s 25ms/step - loss: 0.0615 - accuracy: 0.9889\n",
      "Epoch 7/10\n",
      "122/122 [==============================] - 3s 26ms/step - loss: 0.0439 - accuracy: 0.9923\n",
      "Epoch 8/10\n",
      "122/122 [==============================] - 3s 25ms/step - loss: 0.0385 - accuracy: 0.9936\n",
      "Epoch 9/10\n",
      "122/122 [==============================] - 3s 26ms/step - loss: 0.0285 - accuracy: 0.9951\n",
      "Epoch 10/10\n",
      "122/122 [==============================] - 3s 25ms/step - loss: 0.0838 - accuracy: 0.9750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17bd5bb9dc0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=len(train_word_index)+1, \n",
    "                    output_dim=300, \n",
    "                    weights=[train_embeddings], \n",
    "                    input_length=max_seq_len, \n",
    "                    trainable=False))\n",
    "model.add(Conv1D(filters = 200, kernel_size = 3, padding='valid', activation = 'relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3876, 71)\n",
      "(3876, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 0s 9ms/step - loss: 0.8279 - accuracy: 0.7845\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8279296159744263, 0.7845360636711121]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=X_test, y=y_test, batch_size=None, verbose=1, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNBC data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df1\n",
    "hl1_sequences = tokenizer.texts_to_sequences(df1['Headlines'].tolist())\n",
    "# Pad Sequence\n",
    "hl1_data = pad_sequences(hl1_sequences, maxlen=max_seq_len)\n",
    "y_pred_hl1 = model.predict(hl1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.30593675e-04, 1.06171094e-04, 9.99763191e-01],\n",
       "       [8.76965292e-04, 9.98603165e-01, 5.19873225e-04],\n",
       "       [8.57736841e-02, 5.61626395e-03, 9.08610046e-01],\n",
       "       [2.43258327e-02, 6.52187407e-01, 3.23486745e-01],\n",
       "       [9.72132385e-03, 9.69221056e-01, 2.10576281e-02],\n",
       "       [4.05852776e-03, 9.95160997e-01, 7.80438771e-04],\n",
       "       [2.70037475e-04, 9.99677300e-01, 5.26908443e-05],\n",
       "       [2.91122943e-01, 7.08683908e-01, 1.93108179e-04],\n",
       "       [1.52586075e-02, 5.91985881e-01, 3.92755538e-01],\n",
       "       [7.84892961e-03, 8.89970481e-01, 1.02180615e-01]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_hl1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment_module import cluster_extraction\n",
    "hl_sentiment = cluster_extraction(y_pred_hl1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_sentiment[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1_sequences = tokenizer.texts_to_sequences(df1['Description'].tolist())\n",
    "# Pad Sequence\n",
    "ds1_data = pad_sequences(ds1_sequences, maxlen=max_seq_len)\n",
    "y_pred_ds1 = model.predict(ds1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.2729312e-06, 9.9977845e-01, 2.1337251e-04],\n",
       "       [2.4510868e-04, 9.9876726e-01, 9.8757981e-04],\n",
       "       [1.2591290e-03, 9.9517488e-01, 3.5658858e-03],\n",
       "       [7.2648941e-04, 9.9526066e-01, 4.0129069e-03],\n",
       "       [9.7124004e-01, 9.4486261e-03, 1.9311352e-02],\n",
       "       [2.4510868e-04, 9.9876726e-01, 9.8757981e-04],\n",
       "       [1.4582376e-05, 9.9805403e-01, 1.9314173e-03],\n",
       "       [9.8307067e-01, 1.5465150e-02, 1.4641940e-03],\n",
       "       [1.8112676e-03, 6.0057271e-01, 3.9761603e-01],\n",
       "       [1.6419889e-03, 7.7933502e-01, 2.1902300e-01]], dtype=float32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ds1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 0, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_sentiment = cluster_extraction(y_pred_ds1)\n",
    "ds_sentiment[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 1, 2, 1, 0, 1, 1, 0, 1, 1]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_c_sentiment = combine_sentiments(hl_sentiment, ds_sentiment)\n",
    "cnn_c_sentiment[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'final_df1' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# storing data for the result dataframe\n",
    "%store -r final_df1\n",
    "final_df1['cnn_sentiment'] = cnn_c_sentiment\n",
    "%store final_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuters data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df2\n",
    "hl2_sequences = tokenizer.texts_to_sequences(df2['Headlines'].tolist())\n",
    "# Pad Sequence\n",
    "hl2_data = pad_sequences(hl2_sequences, maxlen=max_seq_len)\n",
    "y_pred_hl2 = model.predict(hl2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.2337724e-05, 9.9997222e-01, 1.5466285e-05],\n",
       "       [9.8997140e-01, 4.3338984e-03, 5.6946725e-03],\n",
       "       [6.0721417e-03, 9.7219616e-01, 2.1731671e-02],\n",
       "       [1.2982792e-01, 8.3164650e-01, 3.8525585e-02],\n",
       "       [8.3935866e-03, 8.5302156e-01, 1.3858491e-01],\n",
       "       [2.3936755e-03, 9.9640840e-01, 1.1979013e-03],\n",
       "       [9.9466473e-04, 9.9819750e-01, 8.0779655e-04],\n",
       "       [1.2796985e-01, 7.4257767e-01, 1.2945250e-01],\n",
       "       [3.2251229e-04, 9.9966753e-01, 9.9892359e-06],\n",
       "       [1.3614590e-01, 1.3370870e-01, 7.3014539e-01]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_hl2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment_module import cluster_extraction\n",
    "hl_sentiment = cluster_extraction(y_pred_hl2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 1, 1, 1, 1, 1, 1, 1, 2]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_sentiment[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds2_sequences = tokenizer.texts_to_sequences(df2['Description'].tolist())\n",
    "# Pad Sequence\n",
    "ds2_data = pad_sequences(ds2_sequences, maxlen=max_seq_len)\n",
    "y_pred_ds2 = model.predict(ds2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.1355215e-02, 6.8908483e-01, 2.8955993e-01],\n",
       "       [4.9470669e-01, 4.0708533e-01, 9.8208003e-02],\n",
       "       [6.2387860e-01, 3.7132826e-01, 4.7931010e-03],\n",
       "       [1.4085230e-02, 9.8535776e-01, 5.5695890e-04],\n",
       "       [3.0549955e-02, 1.8481511e-01, 7.8463489e-01],\n",
       "       [9.7510433e-01, 2.4155397e-02, 7.4017962e-04],\n",
       "       [1.2952825e-01, 2.3824328e-03, 8.6808926e-01],\n",
       "       [1.0938160e-02, 9.8649967e-01, 2.5621513e-03],\n",
       "       [3.0353493e-03, 9.5955366e-01, 3.7410989e-02],\n",
       "       [9.4615109e-03, 1.1750282e-04, 9.9042094e-01]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ds2[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 2, 0, 2, 1, 1, 2]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_sentiment = cluster_extraction(y_pred_ds2)\n",
    "ds_sentiment[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 1, 2, 0, 2, 1, 1, 2]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_r_sentiment = combine_sentiments(hl_sentiment, ds_sentiment)\n",
    "cnn_r_sentiment[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'final_df2' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# storing data for the result dataframe\n",
    "%store -r final_df2\n",
    "final_df2['cnn_sentiment'] = cnn_r_sentiment\n",
    "%store final_df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Guardian data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r df3\n",
    "hl3_sequences = tokenizer.texts_to_sequences(df3['Headlines'].tolist())\n",
    "# Pad Sequence\n",
    "hl3_data = pad_sequences(hl3_sequences, maxlen=max_seq_len)\n",
    "y_pred_hl3 = model.predict(hl3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0293522e-02, 9.8063022e-01, 9.0762125e-03],\n",
       "       [1.6725290e-04, 9.9478418e-01, 5.0486671e-03],\n",
       "       [2.9536288e-05, 9.9613994e-01, 3.8304848e-03],\n",
       "       [3.7760653e-03, 9.5738083e-01, 3.8843133e-02],\n",
       "       [2.8704008e-04, 9.9109811e-01, 8.6148502e-03],\n",
       "       [2.7328570e-05, 9.8566890e-01, 1.4303677e-02],\n",
       "       [1.7643274e-01, 8.2212013e-01, 1.4471314e-03],\n",
       "       [1.8365324e-02, 9.0260494e-01, 7.9029776e-02],\n",
       "       [4.3315251e-04, 9.9953413e-01, 3.2760443e-05],\n",
       "       [8.1195042e-04, 9.9402314e-01, 5.1649171e-03]], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_hl3[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentiment_module import cluster_extraction\n",
    "hl_sentiment = cluster_extraction(y_pred_hl3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hl_sentiment[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn_g_sentiment = hl_sentiment\n",
    "cnn_g_sentiment[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'final_df3' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# storing data for the result dataframe\n",
    "%store -r final_df3\n",
    "final_df3['cnn_sentiment'] = cnn_g_sentiment\n",
    "%store final_df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df1.to_csv(\"./output/cnbc.csv\", index = False)\n",
    "final_df2.to_csv(\"./output/reuters.csv\", index = False)\n",
    "final_df3.to_csv(\"./output/guardian.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
